# ğŸ¦Š IAFOX

**IA Local sem RestriÃ§Ãµes para Desenvolvimento**

IAFOX Ã© um assistente de IA que roda 100% no seu computador, pode editar arquivos localmente e nÃ£o tem as restriÃ§Ãµes de APIs comerciais.

## âœ¨ Funcionalidades

- ğŸ¤– **IA Local** - Roda no seu PC usando Ollama
- ğŸ“ **Gerenciamento de Arquivos** - LÃª, cria e edita arquivos
- ğŸ’» **ExecuÃ§Ã£o de Comandos** - Roda comandos no terminal
- ğŸ§  **Sistema RAG** - Aprende com seus documentos
- ğŸ”’ **100% Privado** - Seus dados nunca saem do seu PC
- ğŸš€ **Sem RestriÃ§Ãµes** - Ajuda com qualquer cÃ³digo

## ğŸ“‹ Requisitos

### Hardware Recomendado
- **GPU**: NVIDIA com 8GB+ VRAM (24GB para modelos grandes)
- **RAM**: 32GB+ (64GB recomendado)
- **Storage**: 50GB+ livre para modelos

### Software
- Windows 10/11 ou Linux
- Python 3.10+
- [Ollama](https://ollama.com/download)
- NVIDIA CUDA (para GPU)

## ğŸš€ InstalaÃ§Ã£o RÃ¡pida (Windows)

### OpÃ§Ã£o 1: Script AutomÃ¡tico

```powershell
# Execute no PowerShell como Administrador
.\scripts\install_windows.ps1
```

### OpÃ§Ã£o 2: InstalaÃ§Ã£o Manual

1. **Instale o Ollama**
   - Baixe em: https://ollama.com/download
   - Execute o instalador

2. **Baixe um modelo**
   ```bash
   # Para GPU com 24GB (RTX 4090)
   ollama pull qwen2.5-coder:32b

   # Para GPU com 12GB
   ollama pull qwen2.5-coder:14b

   # Para GPU com 8GB
   ollama pull qwen2.5-coder:7b
   ```

3. **Instale o IAFOX**
   ```bash
   # Clone o repositÃ³rio
   git clone https://github.com/seu-usuario/IAFOX.git
   cd IAFOX

   # Crie ambiente virtual
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   # source .venv/bin/activate  # Linux

   # Instale dependÃªncias
   pip install -e ".[dev]"
   ```

## ğŸ® Como Usar

### Interface CLI (Terminal)

```bash
# Ative o ambiente virtual
.venv\Scripts\activate

# Inicie o IAFOX
iafox chat

# Ou com modelo especÃ­fico
iafox chat --model qwen2.5-coder:32b

# Ou em um diretÃ³rio especÃ­fico
iafox chat --workspace C:\meus\projetos
```

**Comandos no chat:**
- `/help` - Mostra ajuda
- `/clear` - Limpa conversa
- `/model` - Lista modelos
- `/tree` - Mostra arquivos
- `/exit` - Sai

### Interface Web (Navegador)

```bash
# Ative o ambiente virtual
.venv\Scripts\activate

# Inicie o servidor
python -m uvicorn iafox.web.api:app --host 0.0.0.0 --port 8000

# Ou use o script
start_web.bat  # Windows
```

Acesse: http://localhost:8000

### API REST

```bash
# Chat
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Crie uma funÃ§Ã£o de fibonacci"}'

# Ler arquivo
curl http://localhost:8000/api/files/read?path=main.py

# Listar arquivos
curl http://localhost:8000/api/files?path=.

# Executar comando
curl -X POST http://localhost:8000/api/execute \
  -H "Content-Type: application/json" \
  -d '{"command": "dir"}'
```

## ğŸ§  Sistema RAG (Base de Conhecimento)

O RAG permite que a IA "aprenda" consultando seus documentos antes de responder.

```python
from iafox.rag.knowledge_base import knowledge_base

# Adiciona arquivo
await knowledge_base.add_file("documentacao.pdf")

# Adiciona diretÃ³rio inteiro
await knowledge_base.add_directory("./meus_projetos", pattern="*.py")

# Busca informaÃ§Ãµes
results = await knowledge_base.search("como fazer autenticaÃ§Ã£o")

# ObtÃ©m contexto para o prompt
context = await knowledge_base.get_context("como usar a API")
```

## ğŸ“ Estrutura do Projeto

```
IAFOX/
â”œâ”€â”€ iafox/
â”‚   â”œâ”€â”€ core/           # LÃ³gica principal
â”‚   â”‚   â”œâ”€â”€ agent.py    # Agente IA
â”‚   â”‚   â””â”€â”€ config.py   # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ cli/            # Interface terminal
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ web/            # Interface web
â”‚   â”‚   â”œâ”€â”€ api.py      # API REST
â”‚   â”‚   â””â”€â”€ templates/  # HTML
â”‚   â”œâ”€â”€ llm/            # IntegraÃ§Ã£o Ollama
â”‚   â”‚   â””â”€â”€ ollama_client.py
â”‚   â”œâ”€â”€ files/          # Gerenciamento arquivos
â”‚   â”‚   â””â”€â”€ manager.py
â”‚   â””â”€â”€ rag/            # Base de conhecimento
â”‚       â””â”€â”€ knowledge_base.py
â”œâ”€â”€ scripts/            # Scripts instalaÃ§Ã£o
â”œâ”€â”€ tests/              # Testes
â”œâ”€â”€ data/               # Dados RAG
â””â”€â”€ pyproject.toml
```

## âš™ï¸ ConfiguraÃ§Ã£o

Crie `~/.iafox/config.json`:

```json
{
  "ollama": {
    "host": "http://localhost:11434",
    "model": "qwen2.5-coder:32b",
    "timeout": 300
  },
  "files": {
    "workspace": "C:\\meus\\projetos",
    "max_file_size": 10485760
  },
  "rag": {
    "enabled": true,
    "chunk_size": 1000
  },
  "web": {
    "host": "0.0.0.0",
    "port": 8000
  }
}
```

## ğŸ”§ Modelos Recomendados

| Modelo | VRAM | Qualidade | Velocidade |
|--------|------|-----------|------------|
| qwen2.5-coder:32b | 20GB | â­â­â­â­â­ | â­â­â­ |
| qwen2.5-coder:14b | 10GB | â­â­â­â­ | â­â­â­â­ |
| qwen2.5-coder:7b | 6GB | â­â­â­ | â­â­â­â­â­ |
| deepseek-coder-v2:16b | 12GB | â­â­â­â­ | â­â­â­â­ |
| codestral:22b | 14GB | â­â­â­â­ | â­â­â­ |

## ğŸ› Troubleshooting

### Ollama nÃ£o conecta
```bash
# Verifique se estÃ¡ rodando
ollama serve

# Teste
curl http://localhost:11434/api/tags
```

### Modelo muito lento
- Use modelo menor (7b ou 14b)
- Verifique se GPU estÃ¡ sendo usada
- Feche outros programas

### Erro de memÃ³ria
- Use modelo quantizado (q4_0, q4_k_m)
- Reduza tamanho do contexto
- Feche aplicativos pesados

## ğŸ“œ LicenÃ§a

MIT License - Use como quiser!

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie sua branch (`git checkout -b feature/nova-feature`)
3. Commit (`git commit -m 'Adiciona nova feature'`)
4. Push (`git push origin feature/nova-feature`)
5. Abra um Pull Request

---

Feito com ğŸ¦Š por IAFOX Team
